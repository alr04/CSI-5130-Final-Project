{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8771f1cb-762f-4841-8978-728625734556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dede89e7-0944-4b21-aee4-1a245d98e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChordTypeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, num_samples, device, num_data_items, random_state):\n",
    "        self.num_data_items = num_data_items\n",
    "        self.random_state = random_state\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        if num_data_items < len(self.annotations):\n",
    "            self.annotations = self.annotations.sample(self.num_data_items, random_state=self.random_state)\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sample_rate = torchaudio.load(audio_sample_path)\n",
    "        # Signal -> PyTorch Tensor (num_channels, samples)\n",
    "\n",
    "        # Send signal to device (CUDA or CPU)\n",
    "        signal = signal.to(device)\n",
    "        \n",
    "        # Make the audio signals have a uniform audio rate\n",
    "        signal = self._resample(signal, sample_rate)\n",
    "        # Make the audio mono\n",
    "        signal = self._mixdown(signal)\n",
    "\n",
    "        # Cut the audio to fit the necessary length\n",
    "        signal = self._cut(signal)\n",
    "        # Right pad the audio to fit the necessary length\n",
    "        signal = self._right_pad(signal)\n",
    "\n",
    "        \n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "    def _cut(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:,:self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad(self, signal):\n",
    "        signal_length = signal.shape[1]\n",
    "        if signal_length < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - signal_length\n",
    "            end_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, end_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample(self, signal, original_sample_rate):\n",
    "        # If the current sample rate is not the same as the target sample rate\n",
    "        if original_sample_rate != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(original_sample_rate, self.target_sample_rate).to(device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mixdown(self, signal):\n",
    "        # If the audio is not already mono, make it mono\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdims=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index,1])\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888bcfdf-c400-4b0c-ad3b-86c4ae2adeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNNetwork(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # 4 convolutional blocks / flatten / linear / softmax\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 5 * 9, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # #print(x.shape)\n",
    "        # x = self.conv1(x)\n",
    "        # #print(x.shape)\n",
    "        # x = self.conv2(x)\n",
    "        # #print(x.shape)\n",
    "        # x = self.conv3(x)\n",
    "        # #print(x.shape)\n",
    "        # x = self.conv4(x)\n",
    "        # #print(x.shape)\n",
    "        # x = self.flatten(x)\n",
    "        # #print(x.shape)\n",
    "        # x = self.linear(x)\n",
    "        # #print(x.shape)\n",
    "        # predictions = self.softmax(x)\n",
    "        # #print(predictions.shape)\n",
    "        return self.softmax(self.linear(self.flatten(self.conv4(self.conv3(self.conv2(self.conv1(x)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1476a0-103a-4f8f-bc2d-48f767d17053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sample_input, class_mapping):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(sample_input)\n",
    "        predicted_class = class_mapping[torch.argmax(predictions, dim=1)]\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d3b72f-0f5c-42cd-8fa7-6a9bea78a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_preprocessor(wav_path, transformation, target_sample_rate, num_samples, device):\n",
    "    signal, sample_rate = torchaudio.load(wav_path)\n",
    "    # Signal -> PyTorch Tensor (num_channels, samples)\n",
    "    \n",
    "    # Send signal to device (CUDA or CPU)\n",
    "    signal = signal.to(device)\n",
    "        \n",
    "    # Make the audio signals have a uniform audio rate\n",
    "    # If the current sample rate is not the same as the target sample rate\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate).to(device)\n",
    "        signal = resampler(signal)\n",
    "\n",
    "    # If the audio is not already mono, make it mono\n",
    "    if signal.shape[0] > 1:\n",
    "        signal = torch.mean(signal, dim=0, keepdims=True)\n",
    "\n",
    "    # Cut the audio to fit the necessary length\n",
    "    if signal.shape[1] > num_samples:\n",
    "        signal = signal[:,:num_samples]\n",
    "        \n",
    "    # Right pad the audio to fit the necessary length\n",
    "    signal_length = signal.shape[1]\n",
    "    if signal_length < num_samples:\n",
    "        num_missing_samples = num_samples - signal_length\n",
    "        end_padding = (0, num_missing_samples)\n",
    "        signal = torch.nn.functional.pad(signal, end_padding)\n",
    "\n",
    "    signal = transformation(signal)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32662f7-198d-41b9-8d70-50ced8ab5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chord_type(model_chord, model_root, wav_path, transformation, target_sample_rate, num_samples, type_mapping, root_mapping, device):\n",
    "    input_sample = wav_preprocessor(wav_path, transformation, target_sample_rate, num_samples, device)\n",
    "    input_sample.unsqueeze_(0)\n",
    "\n",
    "    predicted_chord = predict(model_chord, input_sample, type_mapping)\n",
    "    predicted_root = predict(model_chord, input_sample, root_mapping)\n",
    "    return {predicted_chord_type: predicted_chord, predicted_chord_root: predicted_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55797ac5-44a9-4af8-98a4-5661d2bf690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Rate should be 16000 Hz\n",
    "SAMPLE_RATE = 16000\n",
    "# Each item should be 4 seconds long\n",
    "NUM_SAMPLES = 4*SAMPLE_RATE\n",
    "\n",
    "CLASS_MAPPING_CHORD = ['Major', 'Minor', 'Diminished', 'Augmented']\n",
    "CLASS_MAPPING_ROOT = ['Ab', 'A', 'Bb', 'B', 'C', 'Db', 'D', 'Eb', 'E', 'F', 'Gb', 'G']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64).to(device)\n",
    "\n",
    "chord_classifier = MyCNNNetwork(4).to(device)\n",
    "state_dict1 = torch.load(\"50EpochFullChordTypeCNN.pth\", weights_only=True)\n",
    "chord_classifier.load_state_dict(state_dict1)\n",
    "\n",
    "root_classifier = MyCNNNetwork(12).to(device)\n",
    "state_dict2 = torch.load(\"ChordRootCNN.pth\", weights_only=True)\n",
    "root_classifier.load_state_dict(state_dict2)\n",
    "\n",
    "predict_chord = lambda wav_path: predict_chord_type(chord_classifier, root_classifier, wav_path, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, CLASS_MAPPING_CHORD, CLASS_MAPPING_ROOT, device)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column() as predictor:\n",
    "        wav_path = gr.Audio(type=\"filepath\")\n",
    "        chord_predictor = gr.Button(\"Predict Chord\")\n",
    "        predicted_chord_type = gr.Text(label=\"Chord Type\")\n",
    "        predicted_chord_root = gr.Text(label=\"Chord Root\")\n",
    "\n",
    "        chord_predictor.click(fn=predict_chord, inputs=[wav_path], outputs=[predicted_chord_type,predicted_chord_root])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce0de3f9-7c79-444b-be13-2a174c182426",
   "metadata": {},
   "source": [
    "ANNOTATIONS_FILE = 'C:/Users/indys/Desktop/Computer Science Software/Fall2024/Parallel/FinalProject/chord_type.csv'\n",
    "AUDIO_DIR = 'C:/Users/indys/Desktop/Computer Science Software/Fall2024/Parallel/FinalProject/chordDataset'\n",
    "RAND_SEED = 13\n",
    "# Should be = 86400\n",
    "NUM_DATA_ITEMS = 86400\n",
    "\n",
    "torch.manual_seed(RAND_SEED)\n",
    "torch.cuda.manual_seed(RAND_SEED)\n",
    "\n",
    "# Sample Rate should be 16000 Hz\n",
    "SAMPLE_RATE = 16000\n",
    "# Each item should be 4 seconds long\n",
    "NUM_SAMPLES = 4*SAMPLE_RATE\n",
    "\n",
    "CLASS_MAPPING = ['j', 'n', 'd', 'a']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64)\n",
    "\n",
    "# Number of data items should be <= 86400\n",
    "ctd = ChordTypeDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device, NUM_DATA_ITEMS, RAND_SEED)\n",
    "\n",
    "print(f\"There are {len(ctd)} items in the training dataset\")\n",
    "\n",
    "input_sample, target = ctd[900][0], ctd[900][1]\n",
    "input_sample.unsqueeze_(0)\n",
    "\n",
    "\n",
    "chord_classifier = MyCNNNetwork(4).to(device)\n",
    "state_dict = torch.load(\"ChordTypeCNN.pth\", weights_only=True)\n",
    "chord_classifier.load_state_dict(state_dict)\n",
    "\n",
    "predicted = predict(chord_classifier, input_sample, CLASS_MAPPING)\n",
    "print(f\"Predicted: {predicted}\")\n",
    "print(f\"Target: {CLASS_MAPPING[target]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
